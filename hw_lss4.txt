1. Расскажите, как работает регуляризация в решающих деревьях, какие параметры мы штрафуем в данных алгоритмах?
В модели Дерева принятия решений регуляризация работает на ограничение параметров дерева: 
- глубина дерева (max_depth) - чем больше глубина дерева тем вероятнее переобучение, следовательно его нужно ограничить.
- минимальный размер листа (min_sample_leaf) - чем меньше наблюдений в листе тем вероятнее переобучение.

Для моделей Леса принятия решений (RandomForest) параметрами которые нужно ограничивать (контролировать) являются:
- скорость обучения (learning rate) - коэффициент, который показывает, насколько подробно нужно уточнять свои результаты с каждым шагом. Если сделать его слишком низким, то понадобится больше итераций, чтобы прийти к хорошему решению. Но есть риск, что остановиться вовремя не получится, и модель выучит данные слишком подробно, что приведет к переобучению. Если же, напротив, этот параметр будет слишком высоким, модель сможет выйти на хорошую точность за меньшее количество итераций. Но ей будет сложно приблизиться к лучшему результату, и она останется недообученной.
- Отсев (dropout) — параметр, которым задается относительная часть всех данных, скрытая случайным образом во время обучения.

2. По какому принципу рассчитывается "важность признака (feature_importance)" в ансамблях деревьев?
"Важность признаков" позволяют увидеть связь каждого признака с целью прогнозирования - вклад каждого признака в формирование прогнозного значения. Технические подробности этого метода в бустинговых моделях сложны (измеряется среднее уменьшение инородности (the mean and std of accumulation of the impurity decrease within each tree);
Но если сказать проще - важность признака тем больше чем в больших количествах вопросов (в узлах, нодах) признак участвует при продвижении от корня к листьям.